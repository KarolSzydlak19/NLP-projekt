import json
import math
import numpy as np
from collections import deque
from sklearn.preprocessing import StandardScaler
from sklearn.svm import OneClassSVM
from frouros.detectors.concept_drift import DDM, DDMConfig

# KONFIGURACJA
FILE_PATH = (
    "./yelp_academic_dataset_review_embed/yelp_academic_dataset_review_embed.json"
)
CONTEXT_WINDOW = 30
WARMUP_SIZE = 200  # Liczba pr√≥bek do wstƒôpnego nauczenia modelu
DRIFT_THRESHOLD = (
    0.5  # Pr√≥g prawdopodobie≈Ñstwa, powy≈ºej kt√≥rego uznajemy punkt za anomaliƒô
)


def sigmoid(x):
    """
    Mapuje wynik (-inf, +inf) na prawdopodobie≈Ñstwo (0.0, 1.0).
    """
    if x < -500:
        return 0.0
    if x > 500:
        return 1.0
    return 1 / (1 + math.exp(-x))


def process_stream_features_only(file_path):
    """
    Generator czytajƒÖcy plik linia po linii (zak≈Çada posortowane dane).
    """
    print(f"‚è≥ Otwieranie strumienia danych: {file_path}")
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                # Pobieramy dane, obs≈ÇugujƒÖc ewentualne braki p√≥l
                date_str = data.get("date", "N/A")
                text = data.get("text", "")
                stars = data.get("stars", "N/A")
                embedding = np.array(data["embedding"])

                yield embedding, date_str, text, stars
            except json.JSONDecodeError:
                continue


# 1. Model anomalii (SVM + Scaler)
scaler = StandardScaler()
# kernel="rbf" zazwyczaj lepiej mapuje nieliniowe embeddingi ni≈º domy≈õlny
anomaly_model = OneClassSVM(nu=0.1, kernel="rbf", gamma="scale")

# 2. Detektor driftu (Frouros DDM)
# Parametry: warning (ostrze≈ºenie), drift (alarm), min_instances (czas na stabilizacjƒô)
ddm_config = DDMConfig(warning_level=2.0, drift_level=3.0, min_num_instances=30)
drift_detector = DDM(config=ddm_config)

print("üöÄ Rozpoczynam detekcjƒô VIRTUAL DRIFT (Frouros DDM)")

# Zmienne stanu
step = 0
drifts_detected = 0
recent_reviews = deque(maxlen=CONTEXT_WINDOW)
warmup_data = []
model_trained = False

try:
    for embedding, date_str, text, stars in process_stream_features_only(FILE_PATH):
        step += 1

        # FAZA 1: WARMUP (Zbieranie danych do treningu SVM)
        if not model_trained:
            warmup_data.append(embedding)
            if len(warmup_data) >= WARMUP_SIZE:
                print(
                    f"\nüîß Zako≈Ñczono buforowanie. Trenujƒô model bazowy na {WARMUP_SIZE} pr√≥bkach..."
                )
                X_warmup = np.array(warmup_data)

                # Fit scalera i modelu
                scaler.fit(X_warmup)
                X_scaled = scaler.transform(X_warmup)
                anomaly_model.fit(X_scaled)

                model_trained = True
                print("‚úÖ Model wytrenowany. Prze≈ÇƒÖczanie w tryb detekcji online.\n")
            continue

        # FAZA 2: DETEKCJA ONLINE

        # A. Skalowanie pojedynczej pr√≥bki
        X_sample = embedding.reshape(1, -1)
        X_scaled = scaler.transform(X_sample)

        # B. Wynik surowy (Signed Distance) z SVM
        # W sklearn: warto≈õci DODATNIE (+) = Wnƒôtrze (Norma), UJEMNE (-) = Anomalia
        raw_score = anomaly_model.score_samples(X_scaled)[0]

        # C. Normalizacja i Inwersja
        # Chcemy, aby anomalia (ujemny raw_score) dawa≈Ça wysokie prawdopodobie≈Ñstwo (bliskie 1.0).
        # Dlatego u≈ºywamy -raw_score.
        # Przyk≈Çad: raw_score = -5 (anomalia) -> -(-5) = 5 -> sigmoid(5) ~= 0.99
        prob_anomaly = sigmoid(-raw_score)

        # D. Binaryzacja dla DDM
        # DDM dzia≈Ça na strumieniu b≈Çƒôd√≥w (0/1). Uznajemy > 0.5 za "b≈ÇƒÖd" (anomaliƒô statystycznƒÖ).
        is_anomaly = 1 if prob_anomaly > DRIFT_THRESHOLD else 0

        # E. Aktualizacja detektora
        drift_detector.update(value=is_anomaly)

        # F. Logowanie kontekstu do bufora
        recent_reviews.append(
            {
                "step": step,
                "date": date_str,
                "text": text[:100],  # Skracamy tekst dla oszczƒôdno≈õci
                "stars": stars,
                "prob_anomaly": prob_anomaly,
                "is_anomaly": is_anomaly,
            }
        )

        # G. Obs≈Çuga wykrytego Driftu
        if drift_detector.drift:
            drifts_detected += 1
            print(f"\n{'='*80}")
            # [Image of sigmoid function]- conceptual trigger
            print(
                f"üö® [KROK {step} | {date_str}] WYKRYTO CONCEPT DRIFT #{drifts_detected}"
            )
            print(f"   -> Raw SVM Score: {raw_score:.4f} (ujemne warto≈õci to anomalie)")
            print(f"   -> Anomaly Prob:  {prob_anomaly:.4f}")
            print(f"   -> Status:        DRIFT DETECTED")

            # Zapis kontekstu do pliku JSON
            drift_context = {
                "drift_id": drifts_detected,
                "step": step,
                "date": date_str,
                "trigger_prob": prob_anomaly,
                "context": list(recent_reviews),
            }

            fname = f"./logs/drift_{drifts_detected}.json"
            # Upewnij siƒô, ≈ºe katalog ./logs istnieje, lub zapisz w bie≈ºƒÖcym
            try:
                with open(fname, "w", encoding="utf-8") as f:
                    json.dump(drift_context, f, indent=2, ensure_ascii=False)
                print(f"   üíæ Zapisano log: {fname}")
            except FileNotFoundError:
                print(
                    f"   ‚ö†Ô∏è Nie mo≈ºna zapisaƒá logu (sprawd≈∫ czy folder logs istnieje)."
                )

            # Reset detektora po wykryciu dryfu
            drift_detector.reset()

        # Logowanie postƒôpu co 1000 krok√≥w
        if step % 1000 == 0:
            status_msg = "WARNING" if drift_detector.warning else "OK"
            print(
                f"Krok {step} | {date_str} | Prob: {prob_anomaly:.2f} | DDM: {status_msg}"
            )

except KeyboardInterrupt:
    print("\nPrzerwano przez u≈ºytkownika.")
except Exception as e:
    print(f"\n‚ùå B≈ÇƒÖd krytyczny: {e}")

print(f"\n{'='*30}")
print("KONIEC ANALIZY")
print(f"Ca≈Çkowita liczba wykrytych zmian (Concept Drifts): {drifts_detected}")
